{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import datetime\n",
    "import numpy as np\n",
    "import randomcolor\n",
    "import seaborn as sns\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import dask\n",
    "from tqdm.notebook import trange, tqdm, tqdm_notebook\n",
    "colors = list(sns.color_palette('bright'))\n",
    "rng = np.random.default_rng()\n",
    "rng_seed = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base import *\n",
    "from game import *\n",
    "from rules import *\n",
    "from sim import *\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the experiment once"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# EXPERIMENT SETUP\n",
    "#############################################################\n",
    "\n",
    "# RANDOM SEED FOR GENERATING THE GAME\n",
    "# seed = rng_seed.integers(0, 2 ** 24)\n",
    "seed = 9839\n",
    "print(\"Seed: \", seed)\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "###### GAME SETUP\n",
    "n, m = 2, 3\n",
    "# mg = MultilinearGame(np.array([[[1.]],[[-1.]]]).astype(float))\n",
    "# mg = MultilinearGame(np.array([[[1,0,0],[0,1,0],[0,0,1]],[[0,1,0],[0,1,1],[1,0,0]]]))\n",
    "# mg = MultilinearGame(np.array([[[1,0,0],[0,1,0],[0,0,1]],[[0,1,0],[0,0.95,1],[1,0,0]]]))\n",
    "### USE THIS LINE FOR ZERO SUM GAMES\n",
    "# mg = MultilinearGame(zero_sum(rng, size=(m,) * n))\n",
    "### USE THIS LINE FOR GENERAL-SUM GAMES\n",
    "mg = MultilinearGame(rng.uniform(low=-1.0, high=1.0, size=((n,) + (m,) * n)))\n",
    "\n",
    "###### GAME GEOMETRY\n",
    "# my_proj = ProjDimension(np.array([-30],float))\n",
    "# my_proj = ProjDimension(np.array([-.2, -.25, -.33]))\n",
    "my_proj = proj_sim\n",
    "### SEE base.py FOR MORE PROJECTION FUNCTIONS\n",
    "\n",
    "###### LEARNING RATE\n",
    "eta = 0.1\n",
    "### CAN ALSO USE A FUNCTION\n",
    "# eta = lambda t: 1 / math.sqrt(t)\n",
    "\n",
    "\n",
    "\n",
    "###### UPDATE RULES\n",
    "\n",
    "# GD\n",
    "# learner = GradientDescent(lr=eta, proj=my_proj)\n",
    "# learner = BlumMansour(GradientDescent(lr=eta, proj=my_proj)) # BAD\n",
    "\n",
    "# OG EG\n",
    "learner = OptimisticGradient(lr=eta, proj=my_proj)\n",
    "# learner = ExtraGradient(lr=eta, proj=my_proj)\n",
    "# learner = MultiExtraGradient(lr=eta, proj=my_proj, steps=5)\n",
    "# learner = BlumMansour(OptimisticGradient(lr=eta, proj=my_proj))\n",
    "\n",
    "# MWU\n",
    "# learner = MultiplicativeWeightUpdate(lr=eta, proj=my_proj, optimism=0)\n",
    "\n",
    "# OFTRL ODA\n",
    "# learner = OFTRL(lr=eta, proj=my_proj, barrier=l2_regularizer, optimism=1)\n",
    "# learner = OFTRL_l2(lr=eta,proj=my_proj,optimism=0)\n",
    "# learner = OFTRL_l2(lr=eta,proj=my_proj,optimism=1)\n",
    "# learner = ODA_l2(lr=eta,proj=my_proj,optimism=0)\n",
    "# learner = BlumMansour(OFTRL_l2(lr=eta,proj=my_proj))\n",
    "\n",
    "## OFTRL-BM (warning: slow)\n",
    "# learner = OFTRL(lr=eta, proj=my_proj, barrier=log_barrier, optimism=1)\n",
    "# learner = BlumMansour(OFTRL(lr=eta, proj=my_proj, barrier=log_barrier))\n",
    "\n",
    "### UNCOMMENT THE FOLLOWING LINE TO TURN learner INTO ALTERNATING UPDATE\n",
    "# learner = Alternating(learner)\n",
    "\n",
    "\n",
    "\n",
    "# init_state = np.array([[0.1,0.2,0.31],[0.3,0.44,0.22]],float))\n",
    "init_state = np.ones((n,m))\n",
    "\n",
    "sim = GameSim(mg, learner, my_proj, [], init_state=my_proj(init_state))\n",
    "## TRACK SWAP REGRET\n",
    "# sim = GameSim(mg, learner, my_proj, [None, \"swap\"])\n",
    "## NOTE: ONLY USE WITH proj_sim (warning: slow with large m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sim.play(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonnegative : bool. If True, the code will plot max(regret, 0)\n",
    "nonnegative = False\n",
    "\n",
    "# pure: 'last' | None.\n",
    "# If 'last', the regret is compared to the action taken at the last iteration;\n",
    "# if None, the regret is compared to the best action in hindsight at every step.\n",
    "pure = 'last'\n",
    "\n",
    "##################################################\n",
    "# REGRET AND PATH LENGTH PLOTTING\n",
    "##################################################\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(24, 14))\n",
    "(ax1s, (ax_pl, ax_gpl, ax_gdpl)) = axs\n",
    "for ax in axs.flat:\n",
    "    # ax.set_xscale('log')\n",
    "    # ax.set_yscale('log')\n",
    "    pass\n",
    "\n",
    "for i, mode in enumerate([\"comparative\", None, \"swap\"]):\n",
    "    if i == 0 or mode in sim.regret_types:\n",
    "        if i == 0:\n",
    "            regrets = sim.normed_regret_recorder(pure=pure)\n",
    "        else:\n",
    "            regrets = sim.regret_recorders[mode]()\n",
    "        if nonnegative:\n",
    "            regrets = np.maximum(regrets, 0)\n",
    "        ax1s[i].plot([(i + 1) for i in range(regrets.shape[1])], regrets.transpose(), label=[f\"player {i}\" for i in range(regrets.shape[0])])\n",
    "        ax1s[i].legend()\n",
    "        # ax1s[i].plot([(i + 1) for i in range(regrets.shape[1])], [math.sqrt(i + 1) for i in range(regrets.shape[1])])\n",
    "        # plt.xscale(\"log\")\n",
    "        ax1s[i].set_xlabel(\"Time\")\n",
    "        ax1s[i].set_ylabel(f\"{mode if mode else 'External'} regret\")\n",
    "    else:\n",
    "        ax1s[i].axis('off')\n",
    "\n",
    "def temp_func(sim: GameSim):\n",
    "    whole_steps = np.array([a.internal for a in sim.trajectory])\n",
    "    half_steps = np.array([a.state for a in sim.trajectory])\n",
    "    grads = np.array([a.grad for a in sim.trajectory])\n",
    "    gee = ((np.diff(grads, axis=0) ** 2).sum(axis=2) * eta * eta - ((whole_steps - half_steps)[1:,:,:] ** 2).sum(axis=2))\n",
    "    return gee\n",
    "\n",
    "# path_length = temp_func(sim).cumsum(axis=0) / (2 * eta)\n",
    "# ax2.plot([(i + 1) for i in range(path_length.shape[0])], path_length)\n",
    "# ax2.set_xlabel(\"Time\")\n",
    "# ax2.set_ylabel(\"Temp\")\n",
    "#\n",
    "# ax3 = ax1s[3]\n",
    "# def regret_bound(sim: GameSim):\n",
    "#     x_star = np.swapaxes(sim.regret_recorders[None].x_star(),0,1)\n",
    "#     x0 = sim.trajectory[0].state\n",
    "#     xt = np.array([x.state for x in sim.trajectory])\n",
    "#     return ((norm_sq(x0 - x_star, axis=-1) - norm_sq(xt - x_star, axis=-1))[1:] + temp_func(sim).cumsum(axis=0)) / (2 * eta)\n",
    "# reg_bound = regret_bound(sim)\n",
    "# ax3.plot([(i + 1) for i in range(path_length.shape[0])], reg_bound)\n",
    "# ax3.set_xlabel(\"Time\")\n",
    "# ax3.set_ylabel(\"Regret Bound\")\n",
    "\n",
    "### .path_length(order=x, power=y): L-y distance to the x; if x=y=2, it's L2 distance squared\n",
    "path_length = sim.path_length(order=2,power=2)\n",
    "ax_pl.plot([(i + 1) for i in range(path_length.shape[0])], path_length)\n",
    "ax_pl.set_xlabel(\"Time\")\n",
    "ax_pl.set_ylabel(\"Path length\")\n",
    "\n",
    "path_length = sim.grad_path_length(order=2,power=2)\n",
    "ax_gpl.plot([(i + 1) for i in range(path_length.shape[0])], path_length)\n",
    "ax_gpl.set_xlabel(\"Time\")\n",
    "ax_gpl.set_ylabel(\"Grad Path length\")\n",
    "\n",
    "path_length = (lambda t: np.cumsum(np.linalg.norm(t[1:], axis=2) *\n",
    "                                   np.linalg.norm(np.diff(t, axis=0), axis=2),\n",
    "                                   axis=0)) \\\n",
    "    (np.array([a.grad for a in sim.trajectory]))\n",
    "ax_gdpl.plot([(i + 1) for i in range(path_length.shape[0])], path_length)\n",
    "ax_gdpl.set_xlabel(\"Time\")\n",
    "ax_gdpl.set_ylabel(\"Grad*DGrad Path length\")\n",
    "\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RATIO BETWEEN THE PLAYERS\n",
    "\n",
    "path_length = sim.path_length(order=2,power=2)\n",
    "\n",
    "def path_ratio(path_length):\n",
    "    t = path_length.shape[0]\n",
    "    delta = path_length[t-1] - path_length[t // 2]\n",
    "    if np.max(delta) < 1e-9:\n",
    "        return np.max(delta), tuple(delta)\n",
    "    norm_delta = delta / np.max(delta)\n",
    "    return np.max(delta), tuple(norm_delta)\n",
    "\n",
    "path_ratio(path_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### Show trajectories of play\n",
    "# solid line: trajectory of play\n",
    "# dotted line: best response at each step\n",
    "################################################\n",
    "n, m, *_ = mg.weight.shape\n",
    "n1 = math.ceil(math.sqrt(n))\n",
    "n2 = math.ceil(n / n1)\n",
    "fig, axss = plt.subplots(n1, n2, figsize=(18,12))\n",
    "axs = axss.flat\n",
    "x_star = sim.normed_regret_recorder.x_star()\n",
    "for i, ax in enumerate(axs):\n",
    "    if i < n:\n",
    "        tmp = np.array([a[i] for a, *_ in sim.trajectory])\n",
    "        for j in range(m):\n",
    "            ax.plot(tmp[:,j], label=\"action {}\".format(j),color=colors[j])\n",
    "            ax.plot(x_star[i,:,j], linestyle=':',label=\"best resp {}\".format(j),color=colors[j],alpha=0.5)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"Player {}\".format(i))\n",
    "    else:\n",
    "        ax.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCATTER PLOT OF PLAY\n",
    "# similar to Figure 2 of arxiv:1907.04392\n",
    "# only work with n=m=2\n",
    "\n",
    "if False:\n",
    "    x, y = sim.point_trajectory().reshape(-1,2).transpose()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x[::2], y[::2],label=\"player 0\")\n",
    "    ax.scatter(x[1::2], y[1::2],label=\"player 1\")\n",
    "    ax.legend()\n",
    "x, y = sim.point_trajectory()[:,1,:].transpose()\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x[::2], y[::2],label=\"action 0\")\n",
    "ax.scatter(x[1::2], y[1::2],label=\"action 1\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCATTER PLOT OF PLAY in 3D\n",
    "# only work with m=3\n",
    "n, m, *_ = mg.weight.shape\n",
    "if m == 3:\n",
    "    n1 = math.ceil(math.sqrt(n))\n",
    "    n2 = math.ceil(n / n1)\n",
    "    fig, axss = plt.subplots(n1, n2, figsize=(18,12), subplot_kw=dict(projection='3d'))\n",
    "    axs = axss.flat\n",
    "    x_star = sim.normed_regret_recorder.x_star()\n",
    "    for i, ax in enumerate(axs):\n",
    "        if i < n:\n",
    "            tmp = np.array([a[i] for a, *_ in sim.trajectory]).transpose()\n",
    "            ax.elev = 10\n",
    "            ax.azim = 15\n",
    "            ax.plot3D(*tmp)\n",
    "            ax.legend()\n",
    "            ax.set_xlabel(\"action 0\")\n",
    "            ax.set_ylabel(\"action 1\")\n",
    "            ax.set_zlabel(\"action 2\")\n",
    "            ax.set_title(\"Player {}\".format(i))\n",
    "        else:\n",
    "            ax.axis(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_point = np.ones((n,m)) * 0.5\n",
    "pure_regret = sim.normed_regret_recorder(pure=pure_point)\n",
    "pure_traj = sim.point_trajectory()\n",
    "pure_eval = norm_sq(pure_traj - pure_point, axis=-1) / eta + pure_regret.transpose()\n",
    "# pure_eval = norm_sq(pure_traj - pure_point, axis=-1) / eta\n",
    "# pure_eval = -pure_regret.transpose()\n",
    "\n",
    "pure_eval = np.diff(pure_eval, axis=0)\n",
    "\n",
    "pure_eval = pure_eval[:-1:2] + pure_eval[1::2]\n",
    "\n",
    "pure_eval = np.maximum(0, pure_eval)\n",
    "\n",
    "fig, axes = plt.subplots(n, 1, figsize=(18, 12))\n",
    "for i in range(n):\n",
    "    axes.flat[i].plot(pure_eval[:,i])\n",
    "    axes.flat[i].set_title(\"player {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running multiple experiments at once"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use the dask dashbord to monitor process\n",
    "# print(client.dashboard_link)\n",
    "client"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# DO THE EXPERIMENTS AND PLOT ITS PLAY / REGRET\n",
    "######################################################\n",
    "n, m = 2, 3\n",
    "eta = 0.1\n",
    "# my_proj = ProjDimension(np.array([-0.2,-0.2]))\n",
    "my_proj = proj_sim\n",
    "\n",
    "seed_sequence = np.arange(0, 60) + 9821\n",
    "\n",
    "seeds = db.from_sequence(seed_sequence, partition_size=1)\n",
    "def do_experiment(here_seed):\n",
    "    here_rng = np.random.default_rng(here_seed)\n",
    "    # mg = MultilinearGame(zero_sum(here_rng, size=(m,) * n))\n",
    "    mg = MultilinearGame(here_rng.uniform(low=-1.0, high=1.0, size=((n,) + (m,) * n)))\n",
    "    learner = GradientDescent(lr=eta, proj=my_proj)\n",
    "    # learner = MultiplicativeWeightUpdate(lr=eta, proj=my_proj, optimism=0)\n",
    "    # learner = OptimisticGradient(lr=eta,proj=my_proj)\n",
    "    # learner = ExtraGradient(lr=eta,proj=my_proj)\n",
    "    # learner = MultiExtraGradient(lr=eta,proj=my_proj,steps=3)\n",
    "    learner = Alternating(learner)\n",
    "    sim = GameSim(mg, learner, my_proj, [], init_state=my_proj(np.array([[0.1,0.2,0.31],[0.3,0.44,0.22]])))\n",
    "    sim.play(3000)\n",
    "    path_length = sim.path_length(order=2,power=2)\n",
    "    return here_seed, sim, path_length\n",
    "\n",
    "sims = seeds.map(do_experiment)\n",
    "\n",
    "sims = sims.compute()\n",
    "\n",
    "fig, axs = plt.subplots(6, 10, figsize=(28, 18))\n",
    "for i, ax in enumerate(tqdm(axs.flat)):\n",
    "    here_seed, sim, path_length = sims[i]\n",
    "\n",
    "    ### USE THESE 3 LINES TO PLOT REGRET\n",
    "    # regrets = sim.normed_regret_recorder(pure='last')\n",
    "    # ax.plot([(i + 1) for i in range(regrets.shape[1])], regrets.transpose())\n",
    "\n",
    "    ### USE THESE 3 LINES TO PLOT TRAJECTORY OF PLAY\n",
    "    # play_traj = sim.point_trajectory()[:,0,:].transpose()\n",
    "    # ax.plot([(i + 1) for i in range(play_traj.shape[1])], play_traj.transpose())\n",
    "\n",
    "    ### USE THIS TO PLOT PATH LENGTH\n",
    "    ax.plot([(i + 1) for i in range(path_length.shape[0])], path_length)\n",
    "\n",
    "    ax.set_title(\"seed={}\".format(here_seed))\n",
    "    # ax.set_ylim(-5, 8)\n",
    "    ax.tick_params(\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD BRUTE-FORCE CODE TO FIND A BIG REGRET VALUE\n",
    "# outdated. please don't use.\n",
    "\n",
    "mmg = MultilinearGame(rng.uniform(size=((n,) + (m,) * n)))\n",
    "mreg: float = -114514\n",
    "my_proj = proj_l2ball\n",
    "eta = 0.1\n",
    "init_time = datetime.datetime.now()\n",
    "last_output_time = init_time\n",
    "for i in range(114514):\n",
    "    this_output_time = datetime.datetime.now()\n",
    "    if (this_output_time - last_output_time).total_seconds() > 5:\n",
    "        print(\"Iteration {} at {}\".format(i, this_output_time - init_time))\n",
    "        last_output_time = this_output_time\n",
    "    n = 2\n",
    "    m = 10\n",
    "    mg = MultilinearGame(rng.uniform(low=-1.0, high=1.0, size=((n,) + (m,) * n)))\n",
    "    learner = OptimisticGradient(lr=eta, proj=my_proj)\n",
    "    sim = GameSim(mg, learner, proj=my_proj)\n",
    "    sim.play(300)\n",
    "    regrets = sim.regret_recorders['normed']()\n",
    "    reg: float = regrets.max()\n",
    "    if reg > mreg:\n",
    "        mreg = reg\n",
    "        mmg = mg\n",
    "        print(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting games\n",
    "I dump arrays here. Anyone else can safely ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = MultilinearGame(np.array([[[0.67042628, 0.39706957, 0.42964484, 0.61247737],\n",
    "        [0.15414972, 0.20789047, 0.27953194, 0.0516548 ],\n",
    "        [0.22295242, 0.90496207, 0.32473093, 0.35545632],\n",
    "        [0.52555539, 0.38924423, 0.07948948, 0.91891865]],\n",
    "\n",
    "       [[0.8568959 , 0.95730921, 0.33579909, 0.1684858 ],\n",
    "        [0.63496624, 0.43562002, 0.86900799, 0.20695073],\n",
    "        [0.95044942, 0.34200397, 0.98677491, 0.38937916],\n",
    "        [0.52566604, 0.27494413, 0.81210686, 0.91234926]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = MultilinearGame(np.array([[[[0.38725044, 0.34240753, 0.02810337],\n",
    "         [0.55074954, 0.60687508, 0.69976101],\n",
    "         [0.59657795, 0.46950552, 0.27485584]],\n",
    "        [[0.09728714, 0.60672773, 0.54360432],\n",
    "         [0.26454208, 0.25278358, 0.50165633],\n",
    "         [0.04169336, 0.11747752, 0.49429097]],\n",
    "        [[0.77173717, 0.86346229, 0.3505611 ],\n",
    "         [0.35594837, 0.31400333, 0.64525392],\n",
    "         [0.55258491, 0.53210521, 0.04410721]]],\n",
    "       [[[0.73805032, 0.30159735, 0.84759458],\n",
    "         [0.04005955, 0.11278326, 0.84749458],\n",
    "         [0.64340064, 0.04897343, 0.61148755]],\n",
    "        [[0.35718986, 0.67194212, 0.51199374],\n",
    "         [0.39911936, 0.94905635, 0.50967383],\n",
    "         [0.66591209, 0.88617184, 0.55056259]],\n",
    "        [[0.45704226, 0.90936315, 0.90983527],\n",
    "         [0.50159104, 0.18228659, 0.30326444],\n",
    "         [0.3209547 , 0.3873199 , 0.47680925]]],\n",
    "       [[[0.65115376, 0.47580537, 0.35108451],\n",
    "         [0.88925328, 0.10580608, 0.90254684],\n",
    "         [0.09564831, 0.31838851, 0.14622075]],\n",
    "        [[0.17251205, 0.02022372, 0.28844002],\n",
    "         [0.15794837, 0.35324549, 0.1150825 ],\n",
    "         [0.13013662, 0.9649251 , 0.03403977]],\n",
    "        [[0.80036352, 0.65117747, 0.85317148],\n",
    "         [0.52376247, 0.85882519, 0.3877483 ],\n",
    "         [0.96182964, 0.28177822, 0.2208274 ]]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternating GD with n,m = 3,3 that has unbounded regret\n",
    "mg = MultilinearGame(np.array(\n",
    "      [[[[0.88441089, 0.98928826, 0.12753267],\n",
    "         [0.27743761, 0.52243228, 0.31003312],\n",
    "         [0.26097293, 0.41633698, 0.10494305]],\n",
    "        [[0.56297636, 0.55811221, 0.31645741],\n",
    "         [0.47588813, 0.03209457, 0.14239721],\n",
    "         [0.53954066, 0.28642699, 0.42630156]],\n",
    "        [[0.50448122, 0.46060925, 0.37441507],\n",
    "         [0.31501192, 0.59227451, 0.58355955],\n",
    "         [0.07085723, 0.76539987, 0.82268184]]],\n",
    "       [[[0.01907729, 0.37495984, 0.65548201],\n",
    "         [0.07712857, 0.92267657, 0.89250919],\n",
    "         [0.65849824, 0.39492742, 0.1789241 ]],\n",
    "        [[0.71590035, 0.41655805, 0.09372995],\n",
    "         [0.62393117, 0.92962896, 0.71375662],\n",
    "         [0.76087161, 0.18581644, 0.51963984]],\n",
    "        [[0.01223157, 0.15228304, 0.15177192],\n",
    "         [0.63324927, 0.24613835, 0.36513345],\n",
    "         [0.87083472, 0.47730923, 0.87344024]]],\n",
    "       [[[0.35078892, 0.83761125, 0.17646151],\n",
    "         [0.06063313, 0.64906533, 0.31784363],\n",
    "         [0.87107168, 0.02733013, 0.576778  ]],\n",
    "        [[0.17106675, 0.87680747, 0.30420281],\n",
    "         [0.39292541, 0.42737333, 0.6869863 ],\n",
    "         [0.56304928, 0.96096053, 0.75026524]],\n",
    "        [[0.37300235, 0.17974753, 0.63562101],\n",
    "         [0.65430001, 0.73321838, 0.35825595],\n",
    "         [0.96961243, 0.55505106, 0.60931565]]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d30e5f6f6c97a013c3aeb10d96696e894ee31a5990b1ce53ea54389871041b72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
